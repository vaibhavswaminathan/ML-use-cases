{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-af05d2544377>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;31m# linear algebra\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data processing, CSV file I/O\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mft\u001b[0m \u001b[1;31m# language recognition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpycountry\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlanguages\u001b[0m \u001b[1;31m# to convert language ISO to language name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m \u001b[1;31m# for counting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import fasttext as ft # language recognition\n",
    "from pycountry import languages # to convert language ISO to language name\n",
    "from collections import Counter # for counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach for classification of news article:\n",
    "1. Detect languages (other than English) using fasttext.\n",
    "**Done**\n",
    "2. Convert languages other than English to English (using BERT) **Shelved**\n",
    "3. Can we use knn clustering for this unsupervised classification problem. **Shelved**\n",
    "4. Can we get one keyword, preferably the subject part-of-speech, for each document, and given our known output categories, somehow link each keyword to a category? **Done**\n",
    "5. Is it possible to use the vocab tree generated by word2vec as a decision tree for classification? **To do**\n",
    "6. Lastly, display a tSNE plot colour-differentiated by their predicted category **tSNE plot done, yet to classify by category**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-tasks to be improved:\n",
    "\n",
    "1. One keyword appears in more than one topic, while using LDA. **Expected behaviour**: A keyword must occur in only one topic.\n",
    "2. Find optimal number of total topics (LDA)\n",
    "3. Limit number of words per topic (LDA)\n",
    "4. LDA returns topics containing words within documents. How to link words within topics to its general synonyms?\n",
    "<br>\n",
    "<br>\n",
    "Eg: The following sentence,'Whipping up nationalism has a shelf life: Kamal Nath' contains 89% of the topic:\n",
    "<br>\n",
    "<br>\n",
    "0.015*\"back\" + 0.015*\"fighting\" + 0.015*\"homemade\" + 0.015*\"demand\" + 0.015*\"purifier\" + 0.015*\"smog\" + 0.015*\"pakistanis\" + 0.015*\"shelf\" + 0.015*\"whipping\" + 0.015*\"nationalism\".\n",
    "<br>\n",
    "<br>\n",
    "Here, the words 'nationalism' and 'shelf' have been picked up. How to link this to 'politics'?\n",
    "5. Hypertune the parameters of word2vec to produce better word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = 1411104 #number of records in file\n",
    "s = 100000 #desired sample size\n",
    "filename = \"../input/times-internet-news/Train_data (News data set).csv\"\n",
    "skip = sorted(random.sample(range(n),n-s))\n",
    "col_names = ['title','link','description','long_description','id']\n",
    "\n",
    "data = pd.read_csv(filename,sep='|',index_col=None,names=col_names,skiprows=skip)\n",
    "data = data.dropna(how='any')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pecularities about the data\n",
    "\n",
    "1. High number of proper nouns (people, places, brands) in the dataset. Since these proper nouns do not occur too many times, the model must be adept at assigning a relevant class to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT IN USE\n",
    "#using 1/10th of the dataset\n",
    "newsData = df[:df.shape[0]//10]\n",
    "newsData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT IN USE\n",
    "#to split dataset range into buckets of equal size (for batch processing to not overwhelm CPU)\n",
    "total = df.shape[0]\n",
    "nBuckets = 10\n",
    "if total % nBuckets != 0:\n",
    "    remainder = total%nBuckets\n",
    "    total = total - remainder\n",
    "\n",
    "limit = total//nBuckets\n",
    "buckets = []\n",
    "for i in range(1,nBuckets):\n",
    "    buckets.append(i*limit)\n",
    "\n",
    "print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to detect language of title and return list of languages\n",
    "def detect_lang(dataframe, startLimit, endLimit, langs):\n",
    "    for title in dataframe.title[startLimit+1:endLimit]:\n",
    "        title = title.strip()\n",
    "        title = title.replace('\\n','')\n",
    "        lout = lid_model.predict(title)\n",
    "        print(lout)\n",
    "        lstr = ''.join(lout[0])\n",
    "        detected_lang = lstr.split('__')[-1]\n",
    "        if len(detected_lang) == 2:\n",
    "            try:\n",
    "                lang_name = languages.get(alpha_2=detected_lang).name\n",
    "                print(lang_name)\n",
    "                langs.append(lang_name)\n",
    "            except:\n",
    "                langs.append('New')\n",
    "        else:\n",
    "            langs.append('New')\n",
    "    return langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising FastText model\n",
    "lid_model = ft.load_model(\"../input/fasttext-language-prediction/lid.176.bin\")\n",
    "langs = []\n",
    "\n",
    "#detecting title language\n",
    "%time langs = detect_lang(data, -1, data.shape[0], langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(langs))\n",
    "\n",
    "lang_counts = Counter(langs).most_common()\n",
    "for language,count in lang_counts:\n",
    "    print(language,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding column 'title_language' to newsData for title language\n",
    "data['title_language'] = langs\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating list of news content (title + description)\n",
    "\n",
    "data['content'] = data['title']+' '+data['description']\n",
    "contents = data.content[data.title_language=='English']\n",
    "contentList = contents.to_numpy()\n",
    "print(contentList[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword extraction from each title using tf-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer(stop_words='english')\n",
    "word_count_vector=cv.fit_transform(contentList)\n",
    "\n",
    "#display sample 10 words from vocabulary\n",
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#calculating IDF scores for each word\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuple of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "#sample test case\n",
    "doc=contentList[19]\n",
    "\n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    "\n",
    "# now print the results\n",
    "print(\"\\n=====Doc=====\")\n",
    "print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allkeywords = []\n",
    "for iterator in range(len(contentList)):\n",
    "    keywords = []\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([contentList[iterator]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    "    \n",
    "    keywords = [k for k in keywords]\n",
    "    print(keywords)\n",
    "    allkeywords.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#regex for capturing only words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#to initialize list of English stop words\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "#create stemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "#list to add tokenized documents to \n",
    "texts = []\n",
    "\n",
    "for i in contentList:\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    tokens = [i for i in tokens if len(i)>1] #to remove one-letter characters that are created as a result of tokenization\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "#     stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts.append(stopped_tokens)\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "\n",
    "#building Word2Vec model\n",
    "%time w2vmodel = gensim.models.Word2Vec(texts,size=500,window=20,min_count=2,workers=10,iter=10)\n",
    "\n",
    "#saving the model\n",
    "filename = 'word2vec_model'\n",
    "w2vmodel.wv.save(filename)\n",
    "\n",
    "#TRY IF USING pickle\n",
    "# filename = 'finalized_model.sav'\n",
    "# pickle.dump(w2vmodel, open(filename, 'wb'))\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT IN USE\n",
    "import gensim\n",
    "word2vec_path = \"../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and display categories to be used for classification\n",
    "cats = pd.read_csv('../input/time-internet-new-categories/categories_data.csv')\n",
    "cats = cats.dropna(how='any')\n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for category in cats.refined_categories:\n",
    "    category = category.lower()\n",
    "    category = category.split()\n",
    "    category = [k for k in category if len(k)>1]\n",
    "    categories.append(category)\n",
    "\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsimilarities = []\n",
    "for keyword in allkeywords:\n",
    "    category_similarities = []\n",
    "    for category in categories:\n",
    "        similarities=0\n",
    "        for word in category:\n",
    "            for key in keyword:\n",
    "                try:\n",
    "                    similarities += w2vmodel.wv.similarity(key,word)\n",
    "                except:\n",
    "                    similarities += 0\n",
    "        similarity = similarities/(len(category)*len(keyword))\n",
    "        category_similarities.append(round(similarity,3))\n",
    "    allsimilarities.append(category_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample list of similarity values to every category for a news content\n",
    "print('News keywords: \\n',allkeywords[1])\n",
    "print('\\nSimilarities:')\n",
    "for i in range(len(categories)):\n",
    "    print(categories[i],': ',allsimilarities[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_categories = []\n",
    "similarity_score = []\n",
    "for i in range(len(allsimilarities)):\n",
    "    index = allsimilarities[i].index(max(allsimilarities[i]))\n",
    "    text_categories.append(categories[index])\n",
    "    similarity_score.append(allsimilarities[i][index])\n",
    "\n",
    "summary_dict = {'content':contentList,'category':text_categories,'similarity score':similarity_score}\n",
    "summary_df = pd.DataFrame(summary_dict)\n",
    "summary_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth Inverse Frequency for sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "#to give frequency of each word in the corpus/document\n",
    "def map_word_frequency(document):\n",
    "    return Counter(itertools.chain(*document))\n",
    "\n",
    "#to return list of sentence vectors\n",
    "def get_sif_feature_vectors(sentence1, sentence2, word_emb_model=w2vmodel):\n",
    "    sentence1 = [token for token in sentence1.split() if token in word_emb_model.wv.vocab]\n",
    "    sentence2 = [token for token in sentence2.split() if token in word_emb_model.wv.vocab]\n",
    "    print(sentence1)\n",
    "    print(sentence2)\n",
    "    docs = [sentence1, sentence2]\n",
    "    word_counts = map_word_frequency(docs)\n",
    "    print(word_counts)\n",
    "    embedding_size = 500 # size of vectore in word embeddings\n",
    "    a = 0.001\n",
    "    sentence_set=[]\n",
    "    for sentence in [sentence1, sentence2]:\n",
    "        vs = np.zeros(embedding_size)\n",
    "        sentence_length = len(sentence)\n",
    "        print(sentence_length)\n",
    "        for word in sentence:\n",
    "            print(word)\n",
    "            print(word_counts[word])\n",
    "            a_value = a / (a + word_counts[word]) # smooth inverse frequency, SIF\n",
    "            print(a_value)\n",
    "            vs = np.add(vs, np.multiply(a_value, word_emb_model.wv[word])) # vs += sif * word_vector\n",
    "        vs = np.divide(vs, sentence_length) # weighted average\n",
    "        sentence_set.append(vs)\n",
    "    return sentence_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE for word2vec model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "labels = []\n",
    "tokens = []\n",
    "\n",
    "for word in w2vmodel.wv.vocab:\n",
    "    tokens.append(w2vmodel.wv[word])\n",
    "    labels.append(word)\n",
    "    \n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "%time new_values = tsne_model.fit_transform(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "         \n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora,models\n",
    "\n",
    "#creating dictionary by assigning id to each token in our token list\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "#converting our dictionary to bag-of-words model\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#defining number of topics (to be retrived by LDA)\n",
    "num_topics = 3\n",
    "\n",
    "#generating LDA model\n",
    "%time ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                                 id2word=dictionary, \\\n",
    "                                                 passes=4, \\\n",
    "                                                 eta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display results\n",
    "for i,topic in ldamodel.show_topics(formatted=True, num_topics=num_topics):\n",
    "    print(str(i)+\": \\n\"+ topic+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing topics for each document\n",
    "for i in range(len(corpus)):\n",
    "    print(ldamodel[corpus[i]])\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
